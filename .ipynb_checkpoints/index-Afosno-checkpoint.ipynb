{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.pipeline import Pipeline\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('hotel_booking.csv')\n",
    "pre_processed_data=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Algoritmos\n",
    "from importlib import reload\n",
    "reload(Algoritmos)\n",
    "alg=Algoritmos.Algoritmos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Data Exploration and Preparation\n",
    "\n",
    "# Get a Summary of Data\n",
    "describe = data.describe()\n",
    "print(\"\\n\"+ str(describe))\n",
    "\n",
    "# Get information about the data\n",
    "hotel_data_info = data.info()\n",
    "# Size\n",
    "size = data.size\n",
    "print('O tamanho do data set é: ' + str(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first rows of the data set\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last rows\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns are Numeric and which are Categorical\n",
    "dtypes = data.dtypes\n",
    "print(\"Os data types são: \\n\" + str(dtypes) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows\n",
    "linhas = len(data)\n",
    "print(\"Number of rows: \" + str(linhas))\n",
    "\n",
    "# Number of collumns\n",
    "colunas = len(data.columns)\n",
    "print(\"Number of columns: \" + str(colunas))\n",
    "\n",
    "# Collumns Names\n",
    "col_names = data.columns\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of missing values in each column sorted in ascending order\n",
    "print(f\"Number of nulls per column:\\n{data.isnull().sum().sort_values(ascending=False)}\")\n",
    "\n",
    "print(f\"Number of duplicate rows: {data.duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a placeholder for missing values\n",
    "data.fillna(value={'children': -1, 'country': 'Missing', 'agent': 'Missing', 'company': 'No Company'}, inplace=True)\n",
    "data.info()\n",
    "print(f\"\\nNumber of nulls per column:\\n{data.isnull().sum().sort_values(ascending=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if have duplicate rows\n",
    "print(f\"Before dropDuplicates: {(size)}\")\n",
    "data.drop_duplicates(inplace = True)\n",
    "print(f\"After dropDuplicates: {(data.size)}\") # Doesn't have duplicate rows\n",
    "\n",
    "# Check for unique values\n",
    "unique_values = data.nunique()\n",
    "print(f\"\\nUnique values:\\n{unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimnate columns that are not useful\n",
    "data = data.drop(['name','email','phone-number','credit_card', 'reservation_status_date','reservation_status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate columns into categorical and numerical\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Display the separated columns\n",
    "print(\"Categorical Columns:\")\n",
    "print(categorical_columns)\n",
    "print(\"\\nNumerical Columns:\")\n",
    "print(numerical_columns)\n",
    "\n",
    "# Check the data types of each column\n",
    "print(data.dtypes)\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar os dados onde houve cancelamento\n",
    "cancelled_data = data[data['is_canceled'] == 1]\n",
    "\n",
    "# Contagem de cancelamentos por país\n",
    "country_counts = cancelled_data['country'].value_counts()\n",
    "\n",
    "# Selecionar os 15 principais países\n",
    "top_15_countries = country_counts[:15]\n",
    "# Somar as ocorrências dos países restantes e adicionar como \"Others\"\n",
    "others = country_counts[15:].sum()\n",
    "top_15_countries['Others'] = others\n",
    "\n",
    "# Visualização\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title('Top 15 Countries with Reservation Cancellations', color=\"black\")\n",
    "plt.pie(top_15_countries, autopct='%.2f', labels=top_15_countries.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar as colunas categóricas\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Categorical columns: \" + str(categorical_columns))\n",
    "\n",
    "# Criar gráficos lado a lado para cada categoria onde is_canceled == 1 e is_canceled == 0\n",
    "for column in categorical_columns:\n",
    "    # Separar dados cancelados e não cancelados\n",
    "    canceled_data = data[data['is_canceled'] == 1]\n",
    "    non_canceled_data = data[data['is_canceled'] == 0]\n",
    "    \n",
    "    # Checar o número de categorias únicas\n",
    "    unique_values = data[column].nunique()\n",
    "    \n",
    "    # Para colunas com mais de 15 categorias, agrupar valores menores em \"Others\"\n",
    "    if unique_values > 15:\n",
    "        # Cálculo para is_canceled == 1\n",
    "        top_15_canceled = canceled_data[column].value_counts().nlargest(15)\n",
    "        others_canceled = canceled_data[column].value_counts().iloc[15:].sum()\n",
    "        top_15_canceled['Others'] = others_canceled\n",
    "        \n",
    "        # Cálculo para is_canceled == 0\n",
    "        top_15_non_canceled = non_canceled_data[column].value_counts().nlargest(15)\n",
    "        others_non_canceled = non_canceled_data[column].value_counts().iloc[15:].sum()\n",
    "        top_15_non_canceled['Others'] = others_non_canceled\n",
    "\n",
    "        # Gráficos de Barras Lado a Lado\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        sns.barplot(x=top_15_canceled.index, y=top_15_canceled.values, ax=axes[0])\n",
    "        axes[0].set_title(f\"is_canceled - Top 15 ({column})\")\n",
    "        axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "        sns.barplot(x=top_15_non_canceled.index, y=top_15_non_canceled.values, ax=axes[1])\n",
    "        axes[1].set_title(f\"Not_canceled - Top 15 ({column})\")\n",
    "        axes[1].tick_params(axis='x', rotation=90)\n",
    "        \n",
    "    else:\n",
    "        # Gráficos para colunas com 15 ou menos categorias sem \"Others\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        sns.countplot(data=canceled_data, x=column, ax=axes[0])\n",
    "        axes[0].set_title(f\"is_canceled ({column})\")\n",
    "        axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "        sns.countplot(data=non_canceled_data, x=column, ax=axes[1])\n",
    "        axes[1].set_title(f\"Not_canceled ({column})\")\n",
    "        axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_analysis(data, column_name):\n",
    "    describe= data[column_name].describe()\n",
    "    variance_value = data[column_name].var()\n",
    "    iqr_value = data[column_name].quantile(0.75) - data[column_name].quantile(0.25)\n",
    "    skewness_value = data[column_name].skew()\n",
    "    kurtosis_value = data[column_name].kurtosis()\n",
    "\n",
    "    frequency_distribution = data[column_name].value_counts().head(10)\n",
    "\n",
    "     # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Histogram\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(data[column_name], kde=True, bins=30)\n",
    "    plt.title(f'Histogram of {column_name}')\n",
    "\n",
    "    # Box Plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(x=data[column_name])\n",
    "    plt.title(f'Box Plot of {column_name}')\n",
    "\n",
    "   \n",
    "    # Bar Chart (for categorical features)\n",
    "    if data[column_name].dtype == 'int' or data[column_name].dtype == 'object':\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.countplot(x=data[column_name], order=data[column_name].value_counts().index[:10])\n",
    "        plt.title(f'Bar Chart of {column_name} (Top 10)')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "variables_to_analyse=data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "variables_to_analyse = [col for col in variables_to_analyse if col != 'is_canceled']\n",
    "for varible in variables_to_analyse:\n",
    "    univariate_analysis(data, varible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicação dA função FunctionChisq para saber se ha alguma correlação entre colunas categóricas\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "reload(Algoritmos)\n",
    "alg=Algoritmos.Algoritmos()\n",
    "SelectedPredictors = alg.FunctionChisq(data, 'is_canceled', categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentário sobre os Resultados do Teste Qui-Quadrado\n",
    "\n",
    "Os resultados do teste Qui-Quadrado (Chi-Square) revelam como diferentes variáveis categóricas estão relacionadas com a variável `is_canceled`. Abaixo estão as interpretações dos resultados e as implicações dos valores-p obtidos.\n",
    "\n",
    "### Resumo dos Resultados\n",
    "\n",
    "1. **Variáveis Correlacionadas com `is_canceled`**:\n",
    "   - **hotel**: A variável `hotel` tem um valor-p de 0.0, indicando uma correlação extremamente significativa com o cancelamento de reservas. Isso sugere que o tipo de hotel tem um impacto forte na probabilidade de cancelamento.\n",
    "   - **arrival_date_month**: Com um valor-p de \\(3.67 \\times 10^{-119}\\), o mês de chegada é altamente significativo, indicando que os cancelamentos variam significativamente ao longo dos meses.\n",
    "   - **meal**: A variável `meal` apresenta um valor-p de \\(1.32 \\times 10^{-64}\\), sugerindo que o tipo de refeição escolhida tem uma relação importante com os cancelamentos.\n",
    "   - **country**: O país de origem dos hóspedes mostra uma correlação significativa com um valor-p de 0.0, indicando que a nacionalidade pode influenciar a taxa de cancelamento.\n",
    "   - **market_segment**: Com um valor-p de 0.0, essa variável mostra que o segmento de mercado (ex: grupo, individual, etc.) tem uma correlação forte com cancelamentos.\n",
    "   - **distribution_channel**: O canal de distribuição tem um valor-p de 0.0, indicando uma relação significativa com a taxa de cancelamento.\n",
    "   - **reserved_room_type**: Com um valor-p de \\(1.12 \\times 10^{-133}\\), o tipo de quarto reservado tem uma correlação extremamente forte com o cancelamento.\n",
    "   - **assigned_room_type**: Esta variável também apresenta um valor-p de 0.0, indicando que o tipo de quarto designado está fortemente relacionado aos cancelamentos.\n",
    "   - **deposit_type**: O tipo de depósito tem um valor-p de 0.0, sugerindo uma relação significativa com o comportamento de cancelamento.\n",
    "   - **agent**: Com um valor-p de 0.0, a variável que representa o agente de reservas mostra uma correlação forte com o cancelamento.\n",
    "   - **company**: A empresa associada à reserva tem um valor-p de \\(1.74 \\times 10^{-297}\\), indicando uma correlação extremamente forte com a taxa de cancelamento.\n",
    "   - **customer_type**: Com um valor-p de 0.0, o tipo de cliente (ex: novo, recorrente) tem uma relação significativa com cancelamentos.\n",
    "\n",
    "### Conclusões\n",
    "\n",
    "Os resultados indicam que diversas variáveis categóricas estão fortemente correlacionadas com a decisão de cancelar reservas. Esses insights são valiosos para a gestão de reservas em hotéis, permitindo que os profissionais do setor identifiquem padrões e desenvolvam estratégias para reduzir a taxa de cancelamento, como ajustar políticas de reservas e promoções com base no perfil dos hóspedes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar FunctionAnova\n",
    "continuous_columns = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "continuous_columns = [col for col in continuous_columns if col != 'is_canceled']\n",
    "\n",
    "selected_predictors = alg.FunctionAnova(data, \"is_canceled\", continuous_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentário sobre os Resultados da ANOVA\n",
    "\n",
    "Os resultados da análise de variância (ANOVA) fornecem uma visão valiosa sobre como diferentes variáveis contínuas estão relacionadas com a variável categórica `is_canceled`. A seguir, vamos interpretar os resultados e as implicações dos valores-p obtidos.\n",
    "\n",
    "### Resumo dos Resultados\n",
    "\n",
    "1. **Variáveis Correlacionadas com `is_canceled`**:\n",
    "   - **lead_time**: A variável `lead_time` tem um valor-p de 0.0, indicando uma correlação extremamente significativa com o cancelamento de reservas. Isso sugere que quanto maior o tempo de antecedência da reserva, maior a probabilidade de cancelamento.\n",
    "   - **arrival_date_year**: O ano de chegada também se correlaciona com o cancelamento, com um valor-p de \\(8.57 \\times 10^{-9}\\), o que indica uma forte relação.\n",
    "   - **arrival_date_week_number**: Com um valor-p de 0.0049, essa variável mostra que o número da semana do ano tem uma correlação significativa com os cancelamentos.\n",
    "   - **arrival_date_day_of_month**: O dia do mês de chegada também se mostra relevante, com um valor-p de 0.0342, indicando que certos dias podem estar associados a uma maior taxa de cancelamento.\n",
    "   - **stays_in_week_nights**: Essa variável é altamente significativa (valor-p de \\(1.15 \\times 10^{-17}\\)), indicando que a duração da estadia durante a semana influencia a decisão de cancelar.\n",
    "   - **adults**: O número de adultos na reserva tem um valor-p de \\(1.08 \\times 10^{-95}\\), indicando uma relação forte e significativa.\n",
    "   - **babies**: A presença de bebês nas reservas também é significativa (valor-p de \\(2.92 \\times 10^{-29}\\)).\n",
    "   - **is_repeated_guest**: O fato de um hóspede ser repetido tem um valor-p extremamente baixo (\\(2.31 \\times 10^{-189}\\)), sugerindo que hóspedes que já fizeram reservas anteriormente têm uma taxa diferente de cancelamento.\n",
    "   - **previous_cancellations**: O número de cancelamentos anteriores tem um valor-p de \\(8.93 \\times 10^{-319}\\), indicando uma relação muito forte com cancelamentos.\n",
    "   - **previous_bookings_not_canceled**: Essa variável também é relevante (valor-p de \\(1.49 \\times 10^{-87}\\)).\n",
    "   - **booking_changes**: Com um valor-p de 0.0, as mudanças na reserva têm uma correlação significativa.\n",
    "   - **days_in_waiting_list**: Essa variável também é fortemente correlacionada com cancelamentos (valor-p de \\(2.50 \\times 10^{-78}\\)).\n",
    "   - **adr**: O preço médio diário (adr) é significativo com um valor-p de \\(9.68 \\times 10^{-61}\\).\n",
    "   - **required_car_parking_spaces**: A necessidade de espaços de estacionamento é relevante (valor-p de 0.0).\n",
    "   - **total_of_special_requests**: O número total de pedidos especiais também mostra uma correlação significativa (valor-p de 0.0).\n",
    "\n",
    "2. **Variáveis Não Correlacionadas**:\n",
    "   - **stays_in_weekend_nights**: Com um valor-p de 0.5360, essa variável não apresenta uma correlação significativa com cancelamentos.\n",
    "   - **children**: Apesar de seu valor-p de 0.0887 ser relativamente baixo, não é suficiente para considerar uma correlação significativa.\n",
    "\n",
    "### Conclusões\n",
    "\n",
    "Os resultados indicam que várias variáveis contínuas estão significativamente correlacionadas com a decisão de cancelamento de reservas. Esses insights podem ser valiosos para a gestão de reservas em hotéis, permitindo que os profissionais do setor identifiquem padrões e potencialmente desenvolvam estratégias para reduzir a taxa de cancelamento, como ajustar políticas de reservas e promoções baseadas nas características dos hóspedes e nas suas escolhas de reserva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert non-numeric columns to values\n",
    "for column in categorical_columns:\n",
    "    # Convert the column to string type to avoid type conflicts\n",
    "    data[column] = data[column].astype(str)\n",
    "    # Apply LabelEncoder\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['is_canceled'].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the correlation between variables \n",
    "numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "correlation_num = data[numerical_columns].corr()\n",
    "print(correlation_num)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_num, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Extrai os 3 pares mais bem correlacionados (em termos absolutos), sem contar as correlações de uma variável com ela mesma\n",
    "correlation_pairs = correlation_num.unstack().reset_index()\n",
    "correlation_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "correlation_pairs['Correlation'] = correlation_pairs['Correlation'].abs()  # Usa o valor absoluto\n",
    "\n",
    "# Filtra para excluir duplicatas e correlações com a mesma variável\n",
    "correlation_pairs = correlation_pairs[correlation_pairs['Variable 1'] != correlation_pairs['Variable 2']]\n",
    "correlation_pairs = correlation_pairs.drop_duplicates(subset=['Correlation']).sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# Exibe os 3 pares mais bem correlacionados\n",
    "top_3_correlations = correlation_pairs.head(3)\n",
    "print(\"Top 3 most correlated pairs:\\n\", top_3_correlations)\n",
    "\n",
    "# Exibe os 3 pares menos bem correlacionados\n",
    "top_3_correlations = correlation_pairs.tail(3)\n",
    "print(\"Top 3 less correlated pairs:\\n\", top_3_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 2: Univariate Analysis (Distribution of individual features)\n",
    "def bivariate_analysis(data, column_name):\n",
    "    \n",
    "\n",
    "\n",
    "    Objective_col = data.columns[1]\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    sns.barplot(data=data, x=Objective_col, y=column_name)\n",
    "   # plt.savefig(f'bivariate_analysis/bar_plot_{column_name}.png')\n",
    "\n",
    "    sns.displot(data=data, x= column_name, hue=Objective_col, kind=\"kde\")\n",
    "    #plt.savefig(f'bivariate_analysis/dis_plot_kde_{column_name}.png')\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
    "\n",
    "    # Get unique values in the last_col (your categorical variable)\n",
    "    unique_types = data[Objective_col].unique()\n",
    "\n",
    "   \n",
    "   # Loop over each unique type and perform linear regression\n",
    "    for i, t in enumerate(unique_types):\n",
    "        # Filter data for the current type\n",
    "        subset = data[data[Objective_col] == t]\n",
    "        \n",
    "        X = subset[column_name].values.reshape(-1, 1)\n",
    "        y = subset[column_name].values\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        m = model.coef_[0]\n",
    "        b = model.intercept_\n",
    "        \n",
    "        axes[i].scatter(X, y, color='blue', alpha=0.5)\n",
    "        \n",
    "        axes[i].plot(X, model.predict(X), color='red', label=f'y = {m:.2f}x + {b:.2f}')\n",
    "        \n",
    "        # Set the title to show the current type and regression equation\n",
    "        axes[i].set_title(f'{t}')\n",
    "        axes[i].set_xlabel(column_name)\n",
    "\n",
    "    # Set the common ylabel for the whole figure\n",
    "    axes[0].set_ylabel('Values')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.suptitle(f'Scatter Plots and Regression Lines for Each Type in {Objective_col}')\n",
    " \n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f'bivariate_analysis/scatter_plot_{column_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "variables_to_analyse=numerical_columns\n",
    "\n",
    "# Realizar a análise bivariada para cada variável\n",
    "for variable in variables_to_analyse:\n",
    "    bivariate_analysis(data, variable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_columns = data.select_dtypes(include=['object', 'category']).columns\n",
    "# Get the number of unique values for categorical columns\n",
    "unique_values_per_categorical_column = data[categorical_columns].nunique()\n",
    "\n",
    "# Print the result\n",
    "print(unique_values_per_categorical_column)\n",
    "\n",
    "# Function to display the counts of is_canceled for each unique value in the column\n",
    "def display_cancellation_counts(data, column, top_n=10):\n",
    "    unique_vals = data[column].nunique()\n",
    "    \n",
    "    # Group by the column and the 'is_canceled' column, and count occurrences\n",
    "    grouped = data.groupby([column, 'is_canceled']).size().unstack(fill_value=0)\n",
    "    \n",
    "    if unique_vals <= top_n:\n",
    "        # If the number of unique values is small, show all counts\n",
    "        print(f\"\\nColumn: {column}\")\n",
    "        print(grouped)\n",
    "    else:\n",
    "        # If there are many unique values, show the top N values based on their total counts\n",
    "        top_values = data[column].value_counts().head(top_n).index\n",
    "        print(f\"\\nColumn: {column} (Top {top_n} out of {unique_vals} unique values)\")\n",
    "        print(grouped.loc[top_values])\n",
    "\n",
    "# Apply the function for each column except 'is_canceled'\n",
    "for column in categorical_columns:\n",
    "     display_cancellation_counts(data, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bivariate_categorical(data, varaible):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    sns.countplot(data=data, x=variable, hue=data['is_canceled'])\n",
    "    # Add labels and title\n",
    "    plt.xlabel(f'{variable}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Count of is_canceled Type by 'f'{variable}')\n",
    "    plt.legend(title='Is_canceled')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for variable in categorical_columns:\n",
    "    bivariate_categorical(data, variable)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the target variable and features\n",
    "X = pre_processed_data\n",
    "y = data['is_canceled']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the preprocessing steps for numerical and categorical features\n",
    "numerical_features = numerical_columns\n",
    "categorical_features = categorical_columns\n",
    "\n",
    "# Numerical features preprocessing pipeline\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features preprocessing pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine both pipelines into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply the preprocessing to the data\n",
    "pre_processed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "print(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForestClassifier on the pre-processed data\n",
    "\n",
    "# Define the target variable and features\n",
    "X = pre_processed_data\n",
    "y = data['is_canceled']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the RandomForestClassifier using cross-validation\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(rf_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Evaluate the RandomForestClassifier using a confusion matrix\n",
    "\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=rf_classifier.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature importance analysis using the trained RandomForestClassifier\n",
    "\n",
    "# Get feature importances from the RandomForestClassifier\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame to hold feature names and their importance scores\n",
    "feature_names = numerical_features + list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features))\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance scores in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the top 10 most important features\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop(['is_canceled', 'reservation_status'], axis=1)\n",
    "#'company', 'agent','name','email','phone-number','credit_card', 'reservation_status_date'\n",
    "Y=data['is_canceled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bestk=alg.determine_best_K(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values\n",
    "unique_values = data.nunique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_equals2=alg.knn_function(X, Y, 3)\n",
    "k_equals4=alg.knn_function(X, Y, 5)\n",
    "k_equals6=alg.knn_function(X, Y, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_knn5=alg.crossValidation_knn(X, Y, 5, 5)\n",
    "crossval_knn10=alg.crossValidation_knn(X, Y, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost \n",
    "import lightgbm \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define function to create the MLP network\n",
    "def nnetClassif(inputDim, optimizer, neurons):\n",
    "    # Initialize the Sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer + hidden layer\n",
    "    model.add(Dense(units=neurons, \n",
    "                    input_dim=inputDim, \n",
    "                    kernel_initializer='uniform', \n",
    "                    activation='relu'))\n",
    "    \n",
    "    # Second hidden layer\n",
    "    model.add(Dense(units=neurons, \n",
    "                    kernel_initializer='uniform', \n",
    "                    activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(units=1, \n",
    "                    kernel_initializer='uniform', \n",
    "                    activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_scaled.shape[1]  # Number of features in input layer\n",
    "optimizer = 'adam'  # Example optimizer\n",
    "neurons = 10  # Number of neurons in each hidden layer\n",
    "\n",
    "model = nnetClassif(inputDim=input_dim, optimizer=optimizer, neurons=neurons)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=10, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
